## Improving Deep Neural Networks
기본적인 인공신경망은 살펴봤으니, 이제는 인공신경망을 개선하기 위한 방법들을 살펴보자.
이를 통해 신경망의 정확도를 높이고, 과적합을 방지하며, 학습 속도를 높일 수 있다.

> 먼저, 과적합이 뭘까?  

Overfitting은 모델이 훈련 데이터에 지나치게 적응해 새로운 데이터에 대한 일반화 능력이 떨어지는걸 말한다.
Training set에 대한 성능은 잘나오지만, Validation set이나 Test set에 대한 성능이 떨어지는걸 과적합의 징후로 볼 수 있다.
이는 또한, Varaince가 높다고 볼 수 있는데, 이는 모델이 훈련 데이터에 지나치게 민감하게 반응한다는 것을 의미한다.  

반대로 Underfitting은 모델이 너무 단순해서 모든 데이터셋에 대한 성능이 떨어지는 것을 말한다.
이건 Bias가 높다고 말하며, 모델이 너무 단순해서 데이터의 패턴을 잡아내지 못하는 걸 의미한다.

### Regularization
과적합을 방지하기 위한 대표적인 방법으로, 정규화는 모델의 일반화 능력을 높이기 위해 학습에 제약을 가하는 기술이다.
- **L1 Regularization**: 가중치의 절대값에 비례하는 패널티를 부여한다. 이 방식은 불필요한 가중치를 0으로 만들어 모델의 복잡성을 줄일 수 있다.
- **L2 Regularization**: 가중치의 제곱에 비례하는 패널티를 부여한다. 이 방식은 가중치를 작게 유지하면서도 모든 가중치를 0으로 만들진 않아서, 복잡성을 줄이며 모든 가중치를 고려할 수 있다.
- **Dropout**: 훈련 과정에서 일부 노드를 무작위로 비활성화시켜 모델이 특정 노드에 의존하는 것을 방지하고, 모델의 일반화 능력을 향상시킨다. 
- **Early Stopping**: 훈련 과정에서 Validation set의 성능이 떨어지기 시작할 때, 훈련을 중단시켜 과적합을 방지하는 방법이다.
- **Batch Normalization**: 네트워크의 각 층에서 입력 데이터의 분포를 정규화하여, 학습 과정의 안정성을 높이고 학습 속도를 향상시킨다.
    - `BatchNorm1d`: 주로 1차원 데이터에 적용되며, 전형적으로 Fully Connected Layer 또는 1D Convolutional Layer 출력에 사용된다. 예를 들어, 시계열 데이터나 텍스트 데이터가 이에 해당한다.
    - `BatchNorm2d`: 주로 2차원 데이터에 적용되며, 2D Convolutional Layer 출력에 사용된다. 이미지 데이터 처리에 주로 쓰이며, 각 피쳐 맵의 정규화를 수행한다.

정규화 기술을 사용할 때, 하이퍼파라미터를 조정하는 것이 중요하다.
L1, L2 Regularization에서는 패널티의 강도를 조절하는 *lambda* 값(코드 예시에선 weight_decay)을 조절하고, Dropout에서는 비활성화될 노드의 비율을 결정하는 *p* 값(코드 예시에선 dropout_rate)을 조절한다.

정규화 기술을 모델에 적용할 때, 두 가지 접근 방식 중 하나를 사용할 수 있다.
1. **상호 배제적 적용**: 특정 문제에 대해 가장 효과적인 정규화 기술을 찾기 위해 하나씩 독립적으로 적용하고 결과를 비교하는 방법이다. 특히, 새로운 모델을 개발하거나, 알려지지 않은 데이터셋을 사용할 때 이 접근법이 유용하다.
2. **상호 보완적 적용**: 많은 경우에, 여러 정규화 기술을 조합하여 모델에 적용한다. 

상호 배제적 적용으로 시작해서 어떤 정규화 기술이 가장 효과적인지, 또는 특정 조합이 더 나은 결과를 제공하는지 실험하여, 상호 보완적인 적용을 하는 것이 좋아보인다.

[Regularization 코드 예시](./regularization.py)

### Optimization
모델의 주된 목적은 손실 함수의 값을 최소화하는 것이다. 이 과정에서 'partial derivative(편미분)'을 사용하는데, 편미분이란 함수의 한 변수에 대한 미분을 의미한다.
여기서 변수는 손실 함수에 대한 각 가중치와 편향을 말하고, 여기에 대한 기울기를 계산함으로써 어느 방향으로 가야 손실이 감소하는지 알 수 있다.

이 미분 값(기울기)은 해당 파라미터를 조금 변경했을 때, 손실 함수가 얼마나 변화하는지를 나타내고,
기울기의 방향은 손실을 증가시키는 방향을 가리킨다. 따라서, 우리는 기울기의 반대 방향으로 파라미터를 업데이트함으로써 손실을 줄일 수 있다.

[Optimization 코드 예시](./optimization.py)

### Hyperparameter Tuning
모델의 성능을 최대화하기 위해 하이퍼파라미터의 최적값을 찾는 과정이다.
1. **Learning Rate**
2. **Batch Size**
3. **Epochs**
4. **Number of Layers**
5. **Units per Layer**
6. **Dropout Rate**
7. **Patience for Early Stopping**
8. **L1, L2 Regularization Strength**

그 외로 **Optimizer**, **Activation Function**, **Loss Function** 등도 하이퍼파라미터에 속한다.

하이퍼파라미터 튜닝은 다음과 같은 방법으로 수행할 수 있다.
1. **Grid Search**: 가능한 모든 조합을 시도하여 최적의 하이퍼파라미터를 찾는 방법이다. 하지만, 계산 비용이 매우 높다.
2. **Random Search**: 무작위로 하이퍼파라미터를 선택하여 최적의 하이퍼파라미터를 찾는 방법이다. Grid Search보다 더 빠르게 좋은 결과를 도출할 수 있다.
3. **Bayesian Optimization**: 이전 결과를 기반으로 하이퍼파라미터를 선택하여 최적의 하이퍼파라미터를 찾는 방법이다.
4. **Automated Hyperparameter Tuning**: AutoML 라이브러리를 사용하여 최적의 하이퍼파라미터를 찾는 방법이다. 이 방법은 계산 비용이 낮고, 사용하기 쉽다.
5. **Manual Search**: 경험과 직관을 사용하여 하이퍼파라미터를 선택하는 방법이다.

### Model Evaluation and Selection
모델 평가는 모델의 성능을 측정하고 검증하는 과정이다.
1. **Train-Validation-Test Split**: 모델의 일반화 능력을 평가하기 위해서, 데이터셋을 훈련, 검증, 테스트 세트로 나누는 방법이다.
   - 훈련 세트: 모델을 훈련하는 데 사용되는 데이터셋이다.
   - 검증 세트: 하이퍼파라미터 튜닝과 모델 선택을 위해 사용되며, 모델의 성능을 평가하는 데 사용된다.
   - 테스트 세트: 최종 모델의 성능을 평가하기 위해 사용되고, 훈련 과정에서 이 데이터셋은 사용되지 않아야한다.
2. **Cross-Validation**: 데이터의 양이 제한적일 때, 데이터를 최대한 활용하여 모델의 성능을 평가하는 방법이다. 데이터셋을 여러 개의 작은 셋으로 나누고 그 중 하나를 검증에, 나머지는 훈련에 사용한다. 이 과정을 모든 데이터셋에 대해 반복하여, 모델 성능을 종합적으로 평가한다.
3. **Evaluation Metrics**: 모델의 성능을 측정하는 지표이다. 분류 문제에서는 정확도, 정밀도, 재현율, F1 점수, AUC 등이 있고, 회귀 문제에서는 MSE, RMSE, MAE, R-squared 등이 있다.
4. **Bias-Variance Tradeoff**: Bias은 모델이 데이터의 복잡성을 충분히 모델링하지 못하는 경우 발생하고, Variance는 모델이 훈련 데이터에 과하게 적합되어 새로운 데이터에 대한 일반화 능력이 떨어지는 경우 발생하는 오류이다. 높은 편향은 낮은 분산을 의미하고, 그 반대도 마찬가지이다. 즉, 이 둘 사이엔 상충 관계가 있어서 적절한 균형을 맞추는 게 중요하다.

이 외에도 **Ensemble Learning**, **Bootstraping** 등 다양한 개념들을 포함한다.

### Batch Normalization
배치 정규화의 핵심 아이디어는 네트워크의 각 층으로 들어가는 입력 데이터의 분포를 정규화해서, 학습 과정을 안정화하고 가속화하는 것이다.

모델을 훈련시킬 때, 각 배치의 데이터를 사용해서 평균과 분산을 계산하고 이를 이용해 정규화를 하기 때문에 Batch Normalization이라고 부른다.

여기서 Batch는 데이터셋을 여러 작은 부분집합으로 나눈 단위를 의미한다.

### Transfer Learning
전이 학습은 게임에서 체크포인트와 비슷한 개념으로 볼 수 있다. 게임에서 특정 상황에 이르렀을 때 그 상태를 저장하고, 만약 실패하더라도 그 시점으로 돌아가서 다시 시작할 수 있는 것처럼,
전이 학습도 이미 다른 작업을 위해 학습된 모델을 가져와서, 새로운 작업에 맞게 미세 조정을 할 수 있다. 이렇게 하면, 모델이 모든 것을 처음부터 학습할 필요 없이, 특정 지식을 기반으로 새로운 문제를 해결하도록 진화할 수 있다.

데이터 타입별 대표적인 pre-trained model은 다음과 같다.
- **Text Data**: BERT, GPT, TransformerXL 등
- **Image Data**: RestNet, VGG, AlexNet 등
- **Audio Data**: WaveNet, DeepSpeech 등

전이 학습의 구현을 위해선 데이터 전처리가 사전에 필요하므로, 데이터 전처리에 대한 학습을 진행한 뒤에 전이 학습을 구현하도록 하자.