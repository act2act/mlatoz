## Improving Deep Neural Networks
기본적인 인공신경망은 살펴봤으니, 이제는 인공신경망을 개선하기 위한 방법들을 살펴보자.
이를 통해 신경망의 정확도를 높이고, 과적합을 방지하며, 학습 속도를 높일 수 있다.

> 먼저, 과적합이 뭘까?  

Overfitting은 모델이 훈련 데이터에 지나치게 적응해 새로운 데이터에 대한 일반화 능력이 떨어지는걸 말한다.
Training set에 대한 성능은 잘나오지만, Validation set이나 Test set에 대한 성능이 떨어지는걸 과적합의 징후로 볼 수 있다.
이는 또한, Varaince가 높다고 볼 수 있는데, 이는 모델이 훈련 데이터에 지나치게 민감하게 반응한다는 것을 의미한다.  

반대로 Underfitting은 모델이 너무 단순해서 모든 데이터셋에 대한 성능이 떨어지는 것을 말한다.
이건 Bias가 높다고 말하며, 모델이 너무 단순해서 데이터의 패턴을 잡아내지 못하는 걸 의미한다.

### Regularization
과적합을 방지하기 위한 대표적인 방법으로, 정규화는 모델의 일반화 능력을 높이기 위해 학습에 제약을 가하는 기술이다.
- **L1 Regularization**: 가중치의 절대값에 비례하는 패널티를 부여한다. 이 방식은 불필요한 가중치를 0으로 만들어 모델의 복잡성을 줄일 수 있다.
- **L2 Regularization**: 가중치의 제곱에 비례하는 패널티를 부여한다. 이 방식은 가중치를 작게 유지하면서도 모든 가중치를 0으로 만들진 않아서, 복잡성을 줄이며 모든 가중치를 고려할 수 있다.
- **Dropout**: 훈련 과정에서 일부 노드를 무작위로 비활성화시켜 모델이 특정 노드에 의존하는 것을 방지하고, 모델의 일반화 능력을 향상시킨다. 
- **Early Stopping**: 훈련 과정에서 Validation set의 성능이 떨어지기 시작할 때, 훈련을 중단시켜 과적합을 방지하는 방법이다.
- **Batch Normalization**: 네트워크의 각 층에서 입력 데이터의 분포를 정규화하여, 학습 과정의 안정성을 높이고 학습 속도를 향상시킨다.
    - `BatchNorm1d`: 주로 1차원 데이터에 적용되며, 전형적으로 Fully Connected Layer 또는 1D Convolutional Layer 출력에 사용된다. 예를 들어, 시계열 데이터나 텍스트 데이터가 이에 해당한다.
    - `BatchNorm2d`: 주로 2차원 데이터에 적용되며, 2D Convolutional Layer 출력에 사용된다. 이미지 데이터 처리에 주로 쓰이며, 각 피쳐 맵의 정규화를 수행한다.

정규화 기술을 사용할 때, 하이퍼파라미터를 조정하는 것이 중요하다.
L1, L2 Regularization에서는 패널티의 강도를 조절하는 *lambda* 값(코드 예시에선 weight_decay)을 조절하고, Dropout에서는 비활성화될 노드의 비율을 결정하는 *p* 값(코드 예시에선 dropout_rate)을 조절한다.

정규화 기술을 모델에 적용할 때, 두 가지 접근 방식 중 하나를 사용할 수 있다.
1. **상호 배제적 적용**: 특정 문제에 대해 가장 효과적인 정규화 기술을 찾기 위해 하나씩 독립적으로 적용하고 결과를 비교하는 방법이다. 특히, 새로운 모델을 개발하거나, 알려지지 않은 데이터셋을 사용할 때 이 접근법이 유용하다.
2. **상호 보완적 적용**: 많은 경우에, 여러 정규화 기술을 조합하여 모델에 적용한다. 

상호 배제적 적용으로 시작해서 어떤 정규화 기술이 가장 효과적인지, 또는 특정 조합이 더 나은 결과를 제공하는지 실험하여, 상호 보완적인 적용을 하는 것이 좋아보인다.

[Regularization 코드 예시](./regularization.py)

### Optimization

### Hyperparameter Tuning

### Model Evaluation and Selection
